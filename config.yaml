llm:
  provider: "gemini" # The default LLM provider. Can be "openai", "ollama", "gemini", "grok", or "bedrock".

  openai:
    model_name: "gpt-4-turbo"

  ollama:
    base_url: "http://localhost:11434/v1"
    model_name: "llama3.1"

  gemini:
    model_name: "gemini-2.5-flash"

  grok:
    model_name: "llama3-70b-8192"

  bedrock:
    region_name: "us-east-2"
    model_id: "global.amazon.nova-2-lite-v1:0"

marker:
  workers: 4 # Increased for better CPU utilization
  pdftext_workers: 2
  batch_multiplier: 4 # New: Controls how many pages are processed in parallel
  model_precision: "fp16" # New: Use "fp16" for speed on GPU, or "fp32" for max precision
  exclude_images: true # New: Set to true to skip image and table detection for faster processing