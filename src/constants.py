LOG_APP_STARTUP_LOADING = "Application startup: Loading ML models and initializing clients..."
LOG_APP_STARTUP_SUCCESS = "Application startup: Resources loaded successfully."
LOG_APP_SHUTDOWN = "Application shutdown: Cleaning up resources."
LOG_APP_LLM_HEALTH_OK = "LLM is healthy. Starting the main OCR and analysis process..."
LOG_APP_LLM_HEALTH_FAILED = "LLM health check failed. The application cannot proceed."
LOG_APP_LLM_HEALTH_FIX_HINT = "Please review the error logs above to diagnose and fix the configuration or connectivity issue."
LOG_APP_WELCOME = "Welcome to the {title}"

LOG_REQUEST_STARTED = "Request started: {method} {path}"
LOG_REQUEST_FINISHED = "Request finished: {method} {path} - Status {status_code} - Completed in {process_time:.4f}s"
LOG_REQUEST_FAILED = "Request failed: {method} {path}"

LOG_PDF_PROCESSOR_INIT_DEVICE = "Initializing PDFProcessor on device: '{device}'"
LOG_PDF_PROCESSOR_INIT_DONE = "PDFProcessor initialised (Marker models will load on first scanned-PDF encounter)."
LOG_PDF_PROCESSOR_MARKER_LAZY_LOADING = "Scanned PDF detected — loading Marker ML models. This is a one-time cost; subsequent scanned PDFs will be faster."
LOG_PDF_PROCESSOR_LOADING_MODELS = "Loading Marker models... (this may take a while on first run)"
LOG_PDF_PROCESSOR_MODELS_LOADED = "Marker models loaded successfully."
LOG_PDF_CONVERTING = "Converting: {pdf_path}"
LOG_PDF_FP16_CPU_WARNING = "FP16 precision is not recommended for CPU inference; switching to FP32."
LOG_PDF_FILE_NOT_FOUND = "PDF file not found at: {pdf_path}"
LOG_PDF_PDFTEXT_FAILED = "pdftext extraction failed or unavailable: {error}"
LOG_PDF_FAST_PATH = "[FAST PATH] pdftext extracted {char_count} characters — skipping Marker ML models."
LOG_PDF_SLOW_PATH_FALLBACK = "[SLOW PATH] pdftext only found {char_count} chars — PDF appears to be a scan. Falling back to Marker OCR."
LOG_PDF_SLOW_PATH_RUNNING = "[SLOW PATH] Running Marker OCR on: {pdf_path}"

LOG_LLM_SERVICE_INIT = "Initializing {service_name}."
LOG_LLM_API_CALL_FAILED = "{service_name} API call failed: {error}"
LOG_LLM_HEALTH_CHECK_START = "--- Starting LLM Health Check for provider: '{provider}' ---"
LOG_LLM_HEALTH_CHECK_PASSED = "--- LLM Health Check for '{provider}' PASSED ---"
LOG_LLM_HEALTH_CHECK_FAILED = "--- LLM Health Check for '{provider}' FAILED ---"

LOG_HEALTH_API_KEY_MISSING = "❌ {provider_name} API key is not configured in your .env file."
LOG_HEALTH_CHECKING = "Checking {provider} health..."
LOG_HEALTH_PROVIDER_OK = "✅ {provider} API is healthy."
LOG_HEALTH_PROVIDER_ERROR = "❌ An error occurred while checking {provider} health: {error}"
LOG_HEALTH_OPENAI_AUTH_FAILED = "❌ OpenAI authentication failed. Please check your API key."
LOG_HEALTH_CHECKING_OLLAMA = "Checking Ollama health at {base_url}..."
LOG_HEALTH_OLLAMA_UNEXPECTED = "⚠️ Ollama is reachable, but the response was unexpected: {response}"
LOG_HEALTH_OLLAMA_CONNECT_FAILED = "❌ Could not connect to Ollama at {base_url}. Please ensure the service is running."
LOG_HEALTH_GEMINI_KEY_HINT = "Please check your Google API key and ensure the 'Generative Language API' is enabled."
LOG_HEALTH_BEDROCK_UNSUPPORTED_MODEL = "Unsupported Bedrock model for health check: '{model_id}'. No payload available."
LOG_HEALTH_BEDROCK_ASSUMING_IAM = "AWS credentials not set in .env, assuming IAM role or environment variables."
LOG_HEALTH_BEDROCK_OK = "✅ AWS Bedrock is healthy and model '{model_id}' is accessible."
LOG_HEALTH_BEDROCK_NO_CREDENTIALS = "❌ AWS credentials not found. Configure via .env, environment variables, or an IAM role."
LOG_HEALTH_BEDROCK_ACCESS_DENIED = "❌ AWS access denied. The credentials lack 'bedrock:InvokeModel' permission for '{model_id}'."
LOG_HEALTH_BEDROCK_MODEL_NOT_FOUND = "❌ AWS Bedrock model not found or invalid: '{model_id}'. Check the ID and region ('{region}')."
LOG_HEALTH_BEDROCK_CLIENT_ERROR = "❌ An AWS ClientError occurred: {error}"
LOG_HEALTH_BEDROCK_UNEXPECTED_ERROR = "❌ An unexpected error occurred while checking AWS Bedrock health: {error}"

LOG_CLAIM_PROCESS_REQUEST = "Processing insurance claim request."
LOG_INVALID_FILE_TYPE_RECEIVED = "Invalid file type received: {content_type}. Filename: {filename}"
LOG_UPLOADED_FILE_SAVED = "Uploaded file '{filename}' saved to temporary path: {temp_path}"
LOG_PDF_CONVERSION_START = "Starting PDF-to-Markdown conversion for temporary file: {temp_path}"
LOG_PDF_CONVERSION_SUCCESS = "PDF conversion successful. Markdown length: {length} characters."
LOG_LLM_SENDING_MARKDOWN = "Sending markdown to LLM service: {service_name}"
LOG_LLM_RESPONSE_RECEIVED = "LLM response received successfully."
LOG_LLM_JSON_DECODE_FAILED = "Failed to decode JSON from LLM response. Raw response: '{raw_response}'"
LOG_CLAIM_PROCESS_ERROR = "An error occurred during the processing."
LOG_CLEANING_TEMP_FILE = "Cleaning up temporary file: {temp_path}"
LOG_CLAIM_GENERATING_FHIR = "Generating FHIR bundle as part of the initial processing."
LOG_CLAIM_GENERATE_FHIR_FROM_JSON = "Received request to generate FHIR bundle from JSON payload."
LOG_CLAIM_FHIR_MAP_FAILED = "Failed to map JSON to FHIR bundle."
LOG_CLAIM_FHIR_VALIDATION_ERROR = "FHIR validation error"
LOG_CLAIM_BUNDLE_SUMMARY_ERROR = "Bundle summary error"
LOG_CLAIM_EXTRACT_ONLY_ERROR = "Extract-only error"

LOG_FHIR_SNOMED_NOT_FOUND = "snomed_dictionary.json not found. Falling back to raw text extraction."
LOG_FHIR_FLOAT_PARSE_FAILED = "Could not parse '{value}' as float. Using default {default}."
LOG_FHIR_MISSING_REQUIRED_FIELD = "Missing required field '%s' in '%s'. Using fallback: '%s'"
LOG_FHIR_SKIP_ORG_VALIDATION = "Skipping %s resource due to validation error: %s"
LOG_FHIR_SKIP_COST_BLOCK = "Skipping a specificCost block due to missing/invalid data: %s"
LOG_FHIR_SKIP_INSURANCE_PLAN = "Skipping InsurancePlan resource due to validation error: %s"
LOG_FHIR_OWNED_BY_NONE = "CRITICAL: owned_by_ref is None — InsurancePlan will not be built."
FHIR_ORG_CONTEXT_TPA = "TPA Organisation"
FHIR_ORG_CONTEXT_INSURER = "Organisation"

ERROR_CODE_INVALID_FILE_TYPE = "INVALID_FILE_TYPE"
ERROR_MESSAGE_INVALID_FILE_TYPE = "Invalid file type. Only PDFs are accepted."
ERROR_CODE_PROCESSING_ERROR = "PROCESSING_ERROR"
ERROR_MESSAGE_PROCESSING_ERROR = "An unexpected error occurred during processing."
ERROR_MESSAGE_LLM_API_ERROR = "LLM provider API error"
ERROR_MESSAGE_LLM_INVALID_JSON = "LLM did not return a valid JSON object."
ERROR_MESSAGE_LLM_OFFLINE = "LLM_IS_OFFLINE"
ERROR_MESSAGE_LLM_FAILED = "Health check on LLM failed."
ERROR_CODE_FHIR_MAPPING_ERROR = "FHIR_MAPPING_ERROR"
ERROR_MESSAGE_FHIR_MAPPING = "An error occurred during FHIR mapping: {error}"
ERROR_CODE_VALIDATION_ERROR = "VALIDATION_ERROR"
ERROR_CODE_SUMMARY_ERROR = "SUMMARY_ERROR"

LLM_PROVIDER_OPENAI = "openai"
LLM_PROVIDER_OLLAMA = "ollama"
LLM_PROVIDER_GEMINI = "gemini"
LLM_PROVIDER_GROK = "grok"
LLM_PROVIDER_BEDROCK = "bedrock"
LLM_PROVIDERS = [LLM_PROVIDER_OPENAI, LLM_PROVIDER_OLLAMA, LLM_PROVIDER_GEMINI, LLM_PROVIDER_GROK, LLM_PROVIDER_BEDROCK]

HEADER_X_REQUEST_ID = "X-Request-ID"

FE_ERROR_SELECT_FILE = "Please select a file first."
FE_ERROR_API_UNKNOWN = "An unknown error occurred."

LOG_BATCH_PROCESSING_FILE = "▶ Processing {filename}..."
LOG_BATCH_EXTRACTING_TEXT = "  └─ Extracting text..."
LOG_BATCH_PRUNING_TEXT = "  └─ Pruning text..."
LOG_BATCH_SENDING_LLM = "  └─ Sending to LLM..."
LOG_BATCH_PARSING_JSON = "  └─ Parsing JSON..."
LOG_BATCH_GENERATING_FHIR = "  └─ Generating FHIR Bundle..."
LOG_BATCH_FILE_SUCCESS = "✅ Generated {output_filename}"
LOG_BATCH_FILE_FAILED = "❌ Failed to process {filename}: {error}"
LOG_BATCH_INPUT_DIR = "Input directory: {input_dir}"
LOG_BATCH_INPUT_DIR_CREATING = "Input directory does not exist. Creating it now..."
LOG_BATCH_OUTPUT_DIR = "Output directory: {output_dir}"
LOG_BATCH_NO_PDFS = "No PDF files found in the input directory. Drop some PDFs inside and run again."
LOG_BATCH_FOUND_PDFS = "Found {count} PDF files. Initializing components (this might take a moment if starting ML models)..."
LOG_BATCH_SEPARATOR = "=" * 50
LOG_BATCH_START = "Starting batch processing!"
LOG_BATCH_COMPLETE = "Batch processing complete. Successfully generated {success}/{total} bundles."